{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use DSVM to run a Deep learning Keras model to classify fashion items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You created a fashion classification model in [Microsoft Custom Vision service](https://www.customvision.ai/?WT.mc_id=aisummit-github-amynic) via UI and in python code. However if you need to start building your own custom code Deep Neural Networks - [Keras](https://keras.io/) is a great high level API to use to do this.\n",
    "\n",
    "In the code below we build a model from the Fashion MNIST dataset to classify clothing items into 10 categories. The data can be found here: https://github.com/zalandoresearch/fashion-mnist\n",
    "\n",
    "Because we are using the DSVM, all packages and frameworks are already installed on the machine. So we only need to import the package we need! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Import Tensorflow framework and Keras. As well as other more basic python packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You can change the backend used in Keras from Tensorflow to CNTK by simply changing the `os.environ['KERAS_BACKEND']` variable to `cntk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow Version is: 1.11.0\n",
      "tensorflow\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]= \"2\"\n",
    "print(\"tensorflow Version is: \" + str(tf.__version__))\n",
    "\n",
    "import numpy as np\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "from keras import backend as K\n",
    "print(os.environ['KERAS_BACKEND'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Import all the Keras functions we will need to use to create a Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Fashion MNIST Dataset CNN model development: https://github.com/zalandoresearch/fashion-mnist\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import utils, losses, optimizers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We setup some variables for example how many classes there are [0-9] as well as batch size to send the training sample of data in to the model and epochs is how many iterations/run thoroughs of the data there are\n",
    "* Each image is of size 28 x 28 pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#no. of classes\n",
    "num_classes = 10\n",
    "\n",
    "# batch size and training iterations (epochs)\n",
    "batch_size = 128\n",
    "epochs = 24\n",
    "\n",
    "#input image dimensions\n",
    "img_rows,img_cols = 28,28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this section lets have a look at the data\n",
    "* we pull in the fashion MNIST data from the Keras library into training and testing sets\n",
    "* X stands for features and y stands for labels\n",
    "* From the shape statements in the output you can see there are 60,000! training images and 10,000 test images - so a lot more data to use in this model\n",
    "* We then show one of the images from the training set and the corresponding text label for the image\n",
    "\n",
    "> change the img_index field to any number between 0 - 60000 to see different images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) train set\n",
      "(10000, 28, 28) test set\n",
      "Label Index: 9 Fashion Labels: Ankle boot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#data for train and testing\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "print(x_train.shape, 'train set')\n",
    "print(x_test.shape, 'test set')\n",
    "\n",
    "# Define the text labels\n",
    "fashion_mnist_labels = [\"Top\",          # index 0\n",
    "                        \"Trouser\",      # index 1\n",
    "                        \"Jumper\",       # index 2 \n",
    "                        \"Dress\",        # index 3 \n",
    "                        \"Coat\",         # index 4\n",
    "                        \"Sandal\",       # index 5\n",
    "                        \"Shirt\",        # index 6 \n",
    "                        \"Trainer\",      # index 7 \n",
    "                        \"Bag\",          # index 8 \n",
    "                        \"Ankle boot\"]   # index 9\n",
    "\n",
    "img_index=90\n",
    "label_index = y_train[img_index]\n",
    "plt.imshow(x_train[img_index])\n",
    "print('Label Index: ' + str(label_index) + \" Fashion Labels: \" + (fashion_mnist_labels[label_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this section we normalise the data so the pixel values in the image are between 0 - 1 instead of 0 - 255 pixel values. This will help the model to converge and the math becomes easier with smaller numbers\n",
    "* We also [one-hot-encode](https://www.quora.com/What-is-one-hot-encoding-and-when-is-it-used-in-data-science#) the labels in matrices with 0's and 1's in them only. We do this so the model does not deem any category 0-9 in a numeric ranking. For example it won't think that tshirts[0] always come before trousers[1] when actually these are IDs of the classes not something to be evaluated\n",
    "* finally as we are deadling with greyscale images we have a depth number = 1 that might be interpreted different dpending on the framework used (CNTK, Tensorflow etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#type convert and scale the test and training data\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "#one-hot encoding\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "y_test = utils.to_categorical(y_test,  num_classes)\n",
    "\n",
    "#formatting issues for depth of image (greyscale = 1) with different kernels (tensorflow, cntk, etc)\n",
    "if K.image_data_format()== 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0],1,img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols,1)\n",
    "    x_test = x_test.reshape(x_test.shape[0],img_rows, img_cols,1)\n",
    "    input_shape = (img_rows, img_cols,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are able to define the Convolutional Neural Network (CNN) in layers\n",
    "\n",
    "![CNN](./images/cnn.JPG \"CNN\")\n",
    "\n",
    "* This is a **sequential model** meaning every layer passes information forward to the next layer of the network\n",
    "* **1st Convoltuional Layer** - extracts features from data source, these are kernels/filters and feature maps. Feature maps passed to the  next layer. This layer also has a ReLu activation function - Y = max(0, x) this removes any value <0 and prevents vanishing gradients or weights <0\n",
    "* **2nd pooling layer ** - reduces dimensionality, reduce compute and helps with overfitting of the data.\n",
    "* **3rd Convolutional Layer ** -we add a Convoltuional Layer - extracts features from data source, these are kernels/filters and feature maps. Feature maps passed to the  next layer. This layer also has a ReLu activation function - Y = max(0, x) this removes any value <0 and prevents vanishing gradients or weights <0\n",
    "* **4th Pooling Layer ** - reduces dimensionality, reduce compute and helps with overfitting of the data.\n",
    "* **5th/6th Dense fully connected layer with softmax function:** put features together and classify what item of clothing is used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Run some experiments to see how when you change the model below and rerun all the code the accuarcy and model will change:**\n",
    "* add a dropout layer after the first pooling layer and also before the final dense layer: `model.add(Dropout(0.5))`\n",
    "* change the value of dropout between 0 and 1: `model.add(Dropout(X))`\n",
    "* change the 2 Conv2D layer first variable to 32 instead of 64: `model.add(Conv2D(32, kernel_size=(3,3), activation = 'relu'))`\n",
    "* Add padding to each of the Conv2D layers: `model.add(Conv2D(32, kernel_size=(3,3), padding = 'same', activation = 'relu'))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 26, 26, 64)        640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 11, 11, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               204928    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 243,786\n",
      "Trainable params: 243,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Define the CNN model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3,3), activation = 'relu', input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3,3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This code compiles the CNN model and assigns loss/optimiser functions as well as metrics we wish to view\n",
    "* I start a timer so we know how long the model takes to run\n",
    "* Fit the training data to the model using 24 epoches and batches of 64 images. Pass in the test data as your validation set so we can see how the accuracy differs on the training set to the validation set as the model runs through 24 epochs\n",
    "* Finally evaluate the model using the test/validation set\n",
    "\n",
    "> Have a look at what optimisers are available in Keras and see what happens when you change this value: [https://keras.io/optimizers/](https://keras.io/optimizers/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/24\n",
      "49408/60000 [=======================>......] - ETA: 42s - loss: 0.5705 - acc: 0.7948"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-adb8b5bad97c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#train the model and return loss and accuracy for each epoch - history dictionary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mhist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1399\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1400\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#compile - how to measure loss\n",
    "model.compile(loss=losses.categorical_crossentropy, optimizer=optimizers.Adam(), metrics=['accuracy'])\n",
    "\n",
    "#train the model and return loss and accuracy for each epoch - history dictionary\n",
    "start = time.time()\n",
    "hist = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\n",
    "end = time.time()\n",
    "\n",
    "#evaluate the model on the test data\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test Loss: ', score[0])\n",
    "print('Test Accuracy: ', score[1])\n",
    "print('Time to run: ', (end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This code plots the training and validation accuracy across 24 epochs\n",
    "* The training accuracy is often higher but the validation accuracy is deemed a more real world value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epoch_list = list(range(1, len(hist.history['acc']) + 1))\n",
    "plt.plot(epoch_list, hist.history['acc'], epoch_list, hist.history['val_acc'])\n",
    "plt.legend(('Training Accuracy', \"Validation Accuracy\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Margaret Maynard Reid shows us a great way to visualise a sample set output of the test results here: [https://medium.com/tensorflow/hello-deep-learning-fashion-mnist-with-keras-50fcff8cd74a](https://medium.com/tensorflow/hello-deep-learning-fashion-mnist-with-keras-50fcff8cd74a)\n",
    "\n",
    "Run this code to see a set of 15 images from the test set and whether the labels are assigned correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)\n",
    "\n",
    "# Plot a random sample of 10 test images, their predicted labels and ground truth\n",
    "figure = plt.figure(figsize=(20, 8))\n",
    "for i, index in enumerate(np.random.choice(x_test.shape[0], size=15, replace=False)):\n",
    "    ax = figure.add_subplot(3, 5, i + 1, xticks=[], yticks=[])\n",
    "    # Display each image\n",
    "    ax.imshow(np.squeeze(x_test[index]))\n",
    "    predict_index = np.argmax(predictions[index])\n",
    "    true_index = np.argmax(y_test[index])\n",
    "    # Set the title for each image\n",
    "    ax.set_title(\"{} ({})\".format(fashion_mnist_labels[predict_index], \n",
    "                                  fashion_mnist_labels[true_index]),\n",
    "                                  color=(\"green\" if predict_index == true_index else \"red\"))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [myenv]",
   "language": "python",
   "name": "Python [myenv]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
